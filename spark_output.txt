2019-05-08 19:40:25,310 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-05-08 19:40:26,807 INFO spark.SparkContext: Running Spark version 2.4.1
2019-05-08 19:40:26,939 INFO spark.SparkContext: Submitted application: PopularItems
2019-05-08 19:40:27,184 INFO spark.SecurityManager: Changing view acls to: root
2019-05-08 19:40:27,185 INFO spark.SecurityManager: Changing modify acls to: root
2019-05-08 19:40:27,188 INFO spark.SecurityManager: Changing view acls groups to: 
2019-05-08 19:40:27,189 INFO spark.SecurityManager: Changing modify acls groups to: 
2019-05-08 19:40:27,192 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2019-05-08 19:40:28,065 INFO util.Utils: Successfully started service 'sparkDriver' on port 35719.
2019-05-08 19:40:28,144 INFO spark.SparkEnv: Registering MapOutputTracker
2019-05-08 19:40:28,200 INFO spark.SparkEnv: Registering BlockManagerMaster
2019-05-08 19:40:28,217 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-05-08 19:40:28,223 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2019-05-08 19:40:28,258 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c14144f7-e632-49aa-aa44-323b3d5b1fd8
2019-05-08 19:40:28,314 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
2019-05-08 19:40:28,452 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2019-05-08 19:40:28,993 INFO util.log: Logging initialized @8849ms
2019-05-08 19:40:29,316 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-05-08 19:40:29,369 INFO server.Server: Started @9231ms
2019-05-08 19:40:29,448 INFO server.AbstractConnector: Started ServerConnector@266ffa07{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-05-08 19:40:29,449 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2019-05-08 19:40:29,557 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d4ffac4{/jobs,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,583 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b774d7f{/jobs/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,585 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24290637{/jobs/job,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,605 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5be12a07{/jobs/job/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,612 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@371587f2{/stages,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,621 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41caa8ba{/stages/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,626 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fbceef9{/stages/stage,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,639 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52f5955a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a5662db{/stages/pool,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,648 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1dce4e0e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,651 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28c73330{/storage,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56e62c37{/storage/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,656 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fafdc8a{/storage/rdd,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,658 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30fe574f{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,662 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21b1acd3{/environment,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,666 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4651ab46{/environment/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,668 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@673b748a{/executors,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,673 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@253d030d{/executors/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,681 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9c6f44d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,684 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@428675a8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,706 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e25d656{/static,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,712 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48335086{/,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,724 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38c07e4b{/api,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,727 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58a4a44b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,729 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49239202{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-05-08 19:40:29,735 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-master:4040
2019-05-08 19:40:30,153 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2019-05-08 19:40:30,346 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.17.0.8:7077 after 104 ms (0 ms spent in bootstraps)
2019-05-08 19:40:30,757 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190508194030-0002
2019-05-08 19:40:30,811 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20190508194030-0002/0 on worker-20190508193231-172.17.0.13-8881 (172.17.0.13:8881) with 2 core(s)
2019-05-08 19:40:30,856 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20190508194030-0002/0 on hostPort 172.17.0.13:8881 with 2 core(s), 512.0 MB RAM
2019-05-08 19:40:30,876 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43579.
2019-05-08 19:40:30,892 INFO netty.NettyBlockTransferService: Server created on spark-master:43579
2019-05-08 19:40:30,914 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20190508194030-0002/0 is now RUNNING
2019-05-08 19:40:30,922 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-05-08 19:40:31,207 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 43579, None)
2019-05-08 19:40:31,234 INFO storage.BlockManagerMasterEndpoint: Registering block manager spark-master:43579 with 366.3 MB RAM, BlockManagerId(driver, spark-master, 43579, None)
2019-05-08 19:40:31,278 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 43579, None)
2019-05-08 19:40:31,281 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 43579, None)
2019-05-08 19:40:32,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19237878{/metrics/json,null,AVAILABLE,@Spark}
2019-05-08 19:40:32,306 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-05-08 19:40:34,576 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 420.1 KB, free 365.9 MB)
2019-05-08 19:40:34,848 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KB, free 365.9 MB)
2019-05-08 19:40:34,861 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:43579 (size: 37.1 KB, free: 366.3 MB)
2019-05-08 19:40:34,891 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
2019-05-08 19:40:35,477 INFO mapred.FileInputFormat: Total input files to process : 1
2019-05-08 19:40:36,263 INFO spark.SparkContext: Starting job: collect at /tmp/data/spark.py:20
2019-05-08 19:40:36,488 INFO scheduler.DAGScheduler: Registering RDD 3 (groupByKey at /tmp/data/spark.py:12)
2019-05-08 19:40:36,510 INFO scheduler.DAGScheduler: Registering RDD 7 (distinct at /tmp/data/spark.py:14)
2019-05-08 19:40:36,528 INFO scheduler.DAGScheduler: Registering RDD 11 (groupByKey at /tmp/data/spark.py:15)
2019-05-08 19:40:36,544 INFO scheduler.DAGScheduler: Registering RDD 15 (groupByKey at /tmp/data/spark.py:18)
2019-05-08 19:40:36,552 INFO scheduler.DAGScheduler: Got job 0 (collect at /tmp/data/spark.py:20) with 2 output partitions
2019-05-08 19:40:36,554 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at /tmp/data/spark.py:20)
2019-05-08 19:40:36,556 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
2019-05-08 19:40:36,562 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
2019-05-08 19:40:36,594 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /tmp/data/spark.py:12), which has no missing parents
2019-05-08 19:40:36,928 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.9 KB, free 365.8 MB)
2019-05-08 19:40:36,946 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 365.8 MB)
2019-05-08 19:40:36,959 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:43579 (size: 7.0 KB, free: 366.3 MB)
2019-05-08 19:40:36,974 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:40:37,029 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at groupByKey at /tmp/data/spark.py:12) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:40:37,041 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
2019-05-08 19:40:38,870 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.13:37814) with ID 0
2019-05-08 19:40:38,952 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.17.0.13, executor 0, partition 0, PROCESS_LOCAL, 7878 bytes)
2019-05-08 19:40:38,966 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.17.0.13, executor 0, partition 1, PROCESS_LOCAL, 7878 bytes)
2019-05-08 19:40:39,292 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.13:39393 with 93.3 MB RAM, BlockManagerId(0, 172.17.0.13, 39393, None)
2019-05-08 19:40:40,246 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.13:39393 (size: 7.0 KB, free: 93.3 MB)
2019-05-08 19:40:40,810 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.13:39393 (size: 37.1 KB, free: 93.3 MB)
2019-05-08 19:40:43,970 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5002 ms on 172.17.0.13 (executor 0) (1/2)
2019-05-08 19:40:43,983 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5081 ms on 172.17.0.13 (executor 0) (2/2)
2019-05-08 19:40:43,994 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-05-08 19:40:44,014 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43871
2019-05-08 19:40:44,049 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (groupByKey at /tmp/data/spark.py:12) finished in 7.291 s
2019-05-08 19:40:44,054 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:40:44,058 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:40:44,062 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 2, ShuffleMapStage 3, ResultStage 4)
2019-05-08 19:40:44,068 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:40:44,085 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at distinct at /tmp/data/spark.py:14), which has no missing parents
2019-05-08 19:40:44,119 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.8 KB, free 365.8 MB)
2019-05-08 19:40:44,217 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.2 KB, free 365.8 MB)
2019-05-08 19:40:44,230 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:43579 (size: 8.2 KB, free: 366.2 MB)
2019-05-08 19:40:44,237 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:40:44,242 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at distinct at /tmp/data/spark.py:14) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:40:44,245 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
2019-05-08 19:40:44,290 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.17.0.13, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:44,309 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.17.0.13, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:44,436 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.13:39393 (size: 8.2 KB, free: 93.2 MB)
2019-05-08 19:40:44,516 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.17.0.13:37814
2019-05-08 19:40:44,985 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 677 ms on 172.17.0.13 (executor 0) (1/2)
2019-05-08 19:40:45,010 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 742 ms on 172.17.0.13 (executor 0) (2/2)
2019-05-08 19:40:45,020 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-05-08 19:40:45,030 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (distinct at /tmp/data/spark.py:14) finished in 0.918 s
2019-05-08 19:40:45,033 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:40:45,033 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:40:45,033 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 2, ShuffleMapStage 3, ResultStage 4)
2019-05-08 19:40:45,034 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:40:45,037 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/spark.py:15), which has no missing parents
2019-05-08 19:40:45,054 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.7 KB, free 365.8 MB)
2019-05-08 19:40:45,067 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.5 KB, free 365.8 MB)
2019-05-08 19:40:45,071 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:43579 (size: 7.5 KB, free: 366.2 MB)
2019-05-08 19:40:45,076 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:40:45,082 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/spark.py:15) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:40:45,083 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
2019-05-08 19:40:45,087 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 172.17.0.13, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:45,089 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 172.17.0.13, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:45,160 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.13:39393 (size: 7.5 KB, free: 93.2 MB)
2019-05-08 19:40:45,178 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.17.0.13:37814
2019-05-08 19:40:45,348 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 262 ms on 172.17.0.13 (executor 0) (1/2)
2019-05-08 19:40:45,375 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 288 ms on 172.17.0.13 (executor 0) (2/2)
2019-05-08 19:40:45,378 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-05-08 19:40:45,381 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (groupByKey at /tmp/data/spark.py:15) finished in 0.338 s
2019-05-08 19:40:45,381 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:40:45,381 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:40:45,381 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
2019-05-08 19:40:45,381 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:40:45,382 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[15] at groupByKey at /tmp/data/spark.py:18), which has no missing parents
2019-05-08 19:40:45,388 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.0 KB, free 365.8 MB)
2019-05-08 19:40:45,391 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 365.8 MB)
2019-05-08 19:40:45,392 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:43579 (size: 7.6 KB, free: 366.2 MB)
2019-05-08 19:40:45,394 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:40:45,395 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (PairwiseRDD[15] at groupByKey at /tmp/data/spark.py:18) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:40:45,395 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
2019-05-08 19:40:45,399 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 172.17.0.13, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:45,400 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 172.17.0.13, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2019-05-08 19:40:45,460 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.13:39393 (size: 7.6 KB, free: 93.2 MB)
2019-05-08 19:40:45,494 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.17.0.13:37814
2019-05-08 19:40:45,724 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 326 ms on 172.17.0.13 (executor 0) (1/2)
2019-05-08 19:40:45,762 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 363 ms on 172.17.0.13 (executor 0) (2/2)
2019-05-08 19:40:45,771 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-05-08 19:40:45,776 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (groupByKey at /tmp/data/spark.py:18) finished in 0.391 s
2019-05-08 19:40:45,789 INFO scheduler.DAGScheduler: looking for newly runnable stages
2019-05-08 19:40:45,789 INFO scheduler.DAGScheduler: running: Set()
2019-05-08 19:40:45,791 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
2019-05-08 19:40:45,791 INFO scheduler.DAGScheduler: failed: Set()
2019-05-08 19:40:45,792 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (PythonRDD[18] at collect at /tmp/data/spark.py:20), which has no missing parents
2019-05-08 19:40:45,833 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.3 KB, free 365.8 MB)
2019-05-08 19:40:45,845 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KB, free 365.8 MB)
2019-05-08 19:40:45,851 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-master:43579 (size: 5.2 KB, free: 366.2 MB)
2019-05-08 19:40:45,853 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-05-08 19:40:45,859 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[18] at collect at /tmp/data/spark.py:20) (first 15 tasks are for partitions Vector(0, 1))
2019-05-08 19:40:45,859 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
2019-05-08 19:40:45,868 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, 172.17.0.13, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
2019-05-08 19:40:45,869 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, 172.17.0.13, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
2019-05-08 19:40:46,063 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.17.0.13:39393 (size: 5.2 KB, free: 93.2 MB)
2019-05-08 19:40:46,108 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.13:37814
2019-05-08 19:40:46,327 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 458 ms on 172.17.0.13 (executor 0) (1/2)
2019-05-08 19:40:46,341 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 477 ms on 172.17.0.13 (executor 0) (2/2)
2019-05-08 19:40:46,342 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-05-08 19:40:46,348 INFO scheduler.DAGScheduler: ResultStage 4 (collect at /tmp/data/spark.py:20) finished in 0.541 s
2019-05-08 19:40:46,370 INFO scheduler.DAGScheduler: Job 0 finished: collect at /tmp/data/spark.py:20, took 10.104901 s
Co-views generated: 
1 : 5 3 2
5 : 2 3 1
3 : 2 5 1
2 : 5 3 1
Database Recommendations created/updated
Co-views done
2019-05-08 19:40:46,612 INFO server.AbstractConnector: Stopped Spark@266ffa07{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-05-08 19:40:46,625 INFO ui.SparkUI: Stopped Spark web UI at http://spark-master:4040
2019-05-08 19:40:46,644 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
2019-05-08 19:40:46,647 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2019-05-08 19:40:46,926 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2019-05-08 19:40:46,983 INFO memory.MemoryStore: MemoryStore cleared
2019-05-08 19:40:47,001 INFO storage.BlockManager: BlockManager stopped
2019-05-08 19:40:47,095 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
2019-05-08 19:40:47,108 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2019-05-08 19:40:47,186 INFO spark.SparkContext: Successfully stopped SparkContext
2019-05-08 19:40:47,463 INFO util.ShutdownHookManager: Shutdown hook called
2019-05-08 19:40:47,475 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9f87ee45-99b4-4b6a-b5aa-4ce648bd1338
2019-05-08 19:40:47,505 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9f87ee45-99b4-4b6a-b5aa-4ce648bd1338/pyspark-79545959-8dbb-42d9-b258-181be8790184
2019-05-08 19:40:47,513 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-58ac374b-d1f2-45a2-b951-4c9deed50f5e
